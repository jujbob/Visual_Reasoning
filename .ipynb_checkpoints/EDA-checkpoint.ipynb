{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b86cef",
   "metadata": {},
   "source": [
    "# Visual Reasoning Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4cc32",
   "metadata": {},
   "source": [
    "1. Data preprocessing 어떻게 하면될까?\n",
    " - Image feature를 resnet으로 미리 추출해둘까? --> baseline돌리기 까다로움\n",
    " - DataLoader 구성을 어떻게하면 좋을까? --> DataSet class 안에서 feature를 return 할까? 아니면 ResNet을 밖에둘까\n",
    "\n",
    "\n",
    "2. Baseline Model 설계를 어떻게?\n",
    " - Encoder: image encoding 어떻게 하면 될까? Answer Image 3개를 한꺼번에 encoding? 혹은 각각 encoding 한 후 Weight Sum??\n",
    " - Decoder: Answer 후보 중 정답후보를 어떻게 고를까? Similarity 기준? KL 같은 분포기준? Attention 기반 Scoring??\n",
    "\n",
    "\n",
    "3. SOTA Model 설계를 어떻게? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8025aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:18:30.837508Z",
     "start_time": "2022-07-25T08:18:30.833768Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf287b82",
   "metadata": {},
   "source": [
    "## 1. Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c5de51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:29:54.529288Z",
     "start_time": "2022-07-25T08:29:54.518829Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.font_manager as fm\n",
    "#fm.get_fontconfig_fonts()\n",
    "#font_location = './NanumGothic.ttf'\n",
    "#font_location = 'C:/Windows/Fonts/NanumGothic.ttf' # For Windows\n",
    "#font_name = fm.FontProperties(fname=font_location).get_name()\n",
    "#matplotlib.rc('font', family=font_name)\n",
    "\n",
    "%matplotlib inline\n",
    "# 브라우저에서 바로 이미지를 그린다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e687c0a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:30:09.235202Z",
     "start_time": "2022-07-25T08:30:09.228784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnrows,ncols=6,15\\nfig,ax = plt.subplots(nrows,ncols,figsize=(15,6))\\nplt.subplots_adjust(wspace=0, hspace=0) \\nfor i,j in enumerate(malignant[:nrows*ncols]):\\n    fname = os.path.join(imgpath ,j +'.tif')\\n    img = Image.open(fname)\\n    idcol = ImageDraw.Draw(img)\\n    idcol.rectangle(((0,0),(95,95)),outline='red')\\n    plt.subplot(nrows, ncols, i+1) \\n    plt.imshow(np.array(img))\\n    plt.axis('off')\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nrows,ncols=6,15\n",
    "fig,ax = plt.subplots(nrows,ncols,figsize=(15,6))\n",
    "plt.subplots_adjust(wspace=0, hspace=0) \n",
    "for i,j in enumerate(malignant[:nrows*ncols]):\n",
    "    fname = os.path.join(imgpath ,j +'.tif')\n",
    "    img = Image.open(fname)\n",
    "    idcol = ImageDraw.Draw(img)\n",
    "    idcol.rectangle(((0,0),(95,95)),outline='red')\n",
    "    plt.subplot(nrows, ncols, i+1) \n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')\n",
    "    \n",
    "import plotly.express as px\n",
    "\n",
    "# Create grid of sample images \n",
    "grid_size=30\n",
    "rnd_inds=np.random.randint(0,len(train_ts),grid_size)\n",
    "print(\"image indices:\",rnd_inds)\n",
    "\n",
    "x_grid_train=[train_ts[i][0] for i in rnd_inds]\n",
    "y_grid_train=[train_ts[i][1] for i in rnd_inds]\n",
    "\n",
    "x_grid_train=utils.make_grid(x_grid_train, nrow=10, padding=2)\n",
    "print(x_grid_train.shape)\n",
    "    \n",
    "plot_img(x_grid_train,y_grid_train,'Training Subset Examples')\n",
    "image indices: [1438  355 2681 1007 2970 2925  605 1739  270 1641 1080  786 1840   62\n",
    " 2633  734 1712  511 2681 2973  180 2601  500 1959  869    3 2359 3153\n",
    "  345 1307]\n",
    "torch.Size([3, 146, 482])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c84fc7",
   "metadata": {},
   "source": [
    "## 2. Preparation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a0e88e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.399816Z",
     "start_time": "2022-07-25T08:28:54.395693Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args=None):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.distributed = False\n",
    "        self.gpu_id = \"0,1\"\n",
    "        self.HOME_DIR = \"./datasets/\"\n",
    "        self.TASK_NAME = \"similarity1/\"\n",
    "        self.FOLDER_NAME = \"000003/\"\n",
    "        self.IMAGE_LIST = [\n",
    "                \"0d73dee440ef4291ae926fb5cb4ec55e.jpg\", \n",
    "                \"18f827e0d01742d495d3ecbaffb6255a.jpg\", \n",
    "                \"1b243a095866423da8f4a8f19d78ecf4.jpg\",\n",
    "                \"4db9fa5bd947497d91bffd8de3b07e6e.jpg\",\n",
    "                \"95ac10e3608e4abba9407a2a1cae2883.jpg\",\n",
    "                \"a05e9630fd754a249b1fba0be5f386ed.jpg\",\n",
    "                \"ea1f8a29ba464af4b2b393d4bb50d7c0.jpg\"\n",
    "             ]\n",
    "        self.JSON_NAME = \"000003\"+\".json\"\n",
    "        self.input_dim = 512\n",
    "        self.mlp_hidden = 1024\n",
    "\n",
    "\n",
    "#config = Config()\n",
    "#a_image_file = config.HOME_DIR+config.TASK_NAME+config.FOLDER_NAME+config.IMAGE_LIST[0]\n",
    "#a_image = plt.imread(a_image_file)\n",
    "#plt.imshow(a_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b7b3930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.409244Z",
     "start_time": "2022-07-25T08:28:54.402077Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    \n",
    "    home_dir = config.HOME_DIR\n",
    "    task_name = config.TASK_NAME\n",
    "    dir_list = os.listdir(home_dir+task_name)\n",
    "    sample_list = []\n",
    "    \n",
    "    for directory in dir_list:\n",
    "        FOLDER_NAME = directory+\"/\"\n",
    "        JSON_NAME = directory+\".json\"\n",
    "        FILE_PATH = home_dir+task_name+FOLDER_NAME\n",
    "        a_data = json.load(open(FILE_PATH+JSON_NAME))\n",
    "        a_data[\"file_path\"] = FILE_PATH\n",
    "        a_data[\"answer1\"] = [a_data[\"Answers\"][0]]\n",
    "        a_data[\"answer2\"] = [a_data[\"Answers\"][1]]\n",
    "        del a_data[\"Answers\"]\n",
    "        sample_list.append(a_data)\n",
    "        \n",
    "    return sample_list\n",
    "\n",
    "def get_img_argumentation():\n",
    "    #이미지 전처리를 위한 이미지 크기 변환 및 각도조정을 위한 transform 선언\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0333692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.416661Z",
     "start_time": "2022-07-25T08:28:54.410525Z"
    }
   },
   "outputs": [],
   "source": [
    "class Similarity1_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, config=None, transform=None):\n",
    "        self.df = df\n",
    "        self.config = config\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        \n",
    "        target = sample[\"correct_answer_group_ID\"][0]\n",
    "        category = sample[\"category\"]\n",
    "        q_img = sample[\"file_path\"] + sample[\"Questions\"][0][\"images\"][0][\"image_url\"]\n",
    "        a1_img = [sample[\"file_path\"] + ans_img[\"image_url\"] for ans_img in sample[\"answer1\"][0][\"images\"]]\n",
    "        a2_img = [sample[\"file_path\"] + ans_img[\"image_url\"] for ans_img in sample[\"answer2\"][0][\"images\"]]\n",
    "        \n",
    "        q_img_feature = Image.open(q_img).convert('RGB')  #이미지 데이터를 RGB형태로 읽음 \n",
    "        q_img_feature = self.transform(q_img_feature)  #이미지 데이터의 크기 및 각도등을 변경\n",
    "        \n",
    "        a1_img_feature = [self.transform(Image.open(q_img).convert('RGB')) for img in a1_img]\n",
    "        a2_img_feature = [self.transform(Image.open(q_img).convert('RGB')) for img in a1_img]\n",
    "        \n",
    "        return {\n",
    "            \"target\": target,\n",
    "            \"q_img\": q_img_feature,\n",
    "            \"a1_imgs\": a1_img_feature,\n",
    "            \"a2_imgs\": a2_img_feature\n",
    "        }\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "007718eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.428803Z",
     "start_time": "2022-07-25T08:28:54.419343Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_sequential(in_channels, out_channels, *args, **kwargs):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "           nn.BatchNorm2d(out_channels),\n",
    "           nn.ReLu(),\n",
    "           nn.MaxPool2d(*args, **kwargs))\n",
    "\n",
    "class VRSimilarity(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VRSimilarity, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.backborne = torchvision.models.resnet50(pretrained=True)\n",
    "        self.backborne.fc = nn.Linear(self.backborne.fc.in_features, self.config.input_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(self.config.input_dim*4, self.config.mlp_hidden),\n",
    "                    #nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(self.config.mlp_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, samples):\n",
    "        \n",
    "        #Question Image Feature\n",
    "        q = self.backborne(samples[\"q_img\"])\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        a1_img1 = self.backborne(samples[\"a1_imgs\"][0])\n",
    "        a1_img2 = self.backborne(samples[\"a1_imgs\"][1])\n",
    "        a1_img3 = self.backborne(samples[\"a1_imgs\"][2])\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        a2_img1 = self.backborne(samples[\"a2_imgs\"][0])\n",
    "        a2_img2 = self.backborne(samples[\"a2_imgs\"][1])\n",
    "        a2_img3 = self.backborne(samples[\"a2_imgs\"][2])\n",
    "        \n",
    "        q_a1 = torch.cat([q, a1_img1, a1_img2, a1_img3], axis=1)\n",
    "        q_a2 = torch.cat([q, a2_img1, a2_img2, a2_img3], axis=1)\n",
    "        \n",
    "        q_a1_logit = self.fc(q_a1)\n",
    "        q_a2_logit = self.fc(q_a2)\n",
    "        \n",
    "        return {\n",
    "            \"q_a1_logit\": q_a1_logit,\n",
    "            \"q_a2_logit\": q_a2_logit\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a32563ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.438658Z",
     "start_time": "2022-07-25T08:28:54.430230Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, loss_fn, config):\n",
    "    \n",
    "    total_count_correct = 0\n",
    "    total_num_example = 0\n",
    "    total_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    device = config.device\n",
    "    \n",
    "    for batch in tqdm(train_loader):          \n",
    "        \n",
    "        print(batch[\"q_img\"].size())\n",
    "        \n",
    "        #Question Image Feature\n",
    "        batch[\"q_img\"] = batch[\"q_img\"].to(device)\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        batch[\"a1_imgs\"][0] = batch[\"a1_imgs\"][0].to(device)\n",
    "        batch[\"a1_imgs\"][1] = batch[\"a1_imgs\"][1].to(device)\n",
    "        batch[\"a1_imgs\"][2] = batch[\"a1_imgs\"][2].to(device)\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        batch[\"a2_imgs\"][0] = batch[\"a2_imgs\"][0].to(device)\n",
    "        batch[\"a2_imgs\"][1] = batch[\"a2_imgs\"][1].to(device)\n",
    "        batch[\"a2_imgs\"][2] = batch[\"a2_imgs\"][2].to(device)\n",
    "        logits = model(batch)\n",
    "\n",
    "        target_a1, target_a2 = batch[\"target\"].float().to(device), (batch[\"target\"] == 0).type(torch.float).to(device)\n",
    "        print(logits[\"q_a1_logit\"], logits[\"q_a1_logit\"].size())\n",
    "        print(target_a1, target_a1.size())\n",
    "        #loss_a1 = loss_fn(logits[\"q_a1_logit\"].squeeze() if (len(logits[\"q_a1_logit\"].shape) >= 2) else logits[\"q_a1_logit\"], target_a1)\n",
    "        #loss_a2 = loss_fn(logits[\"q_a2_logit\"].squeeze() if (len(logits[\"q_a2_logit\"].shape) >= 2) else logits[\"q_a2_logit\"], target_a2)\n",
    "        loss_a1 = loss_fn(logits[\"q_a1_logit\"].squeeze(), target_a1)\n",
    "        loss_a2 = loss_fn(logits[\"q_a2_logit\"].squeeze(), target_a2)\n",
    "        print(loss_a1.item())\n",
    "        \n",
    "        loss = loss_a1 + loss_a2\n",
    "        #print(loss.item())\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        predicted_a1 = (torch.sigmoid(logits[\"q_a1_logit\"]) > 0.5).float()\n",
    "        total_count_correct = total_count_correct + torch.sum(predicted_a1.squeeze() == target_a1).item()\n",
    "        total_num_example = total_num_example + target_a1.size(0)\n",
    "        \n",
    "        predicted_a2 = (torch.sigmoid(logits[\"q_a2_logit\"]) > 0.5).float()\n",
    "        total_count_correct = total_count_correct + torch.sum(predicted_a2.squeeze() == target_a2).item()\n",
    "        total_num_example = total_num_example + target_a2.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"LOSS:\", str(sum(total_loss)/total_num_example) + \" Accuracy: \" + str(total_count_correct/total_num_example) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5fa88af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.947850Z",
     "start_time": "2022-07-25T08:28:54.439924Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "sample_list = get_data(config)\n",
    "df = pd.DataFrame(sample_list)\n",
    "df.drop(df[df[\"doc_id\"] == \"000769\"].index, inplace=True)\n",
    "df.drop(df[df[\"doc_id\"] == \"000256\"].index, inplace=True)\n",
    "train_df, test_df = train_test_split(df)\n",
    "transform = get_img_argumentation()\n",
    "train_datasets = Similarity1_Dataset(train_df, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=2)\n",
    "\n",
    "vrs1_model = VRSimilarity(config)\n",
    "#vrs1_model = torch.nn.DataParallel(vrs1_model)\n",
    "vrs1_model = vrs1_model.to(config.device)\n",
    "if config.distributed:\n",
    "    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.gpu])    \n",
    "    model = torch.nn.parallel.DistributedDataParallel(model)    \n",
    "\n",
    "optimizer = torch.optim.Adam(vrs1_model.parameters(), lr=0.0001)\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "083f86b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:58.831991Z",
     "start_time": "2022-07-25T08:28:54.949198Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1184 [00:00<04:11,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0660],\n",
      "        [-0.0513]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7229087352752686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/1184 [00:00<04:30,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1724],\n",
      "        [0.3977]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.5623070001602173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/1184 [00:01<07:47,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0939],\n",
      "        [-0.0399]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7272586822509766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/1184 [00:01<06:27,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4067],\n",
      "        [-0.2466]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.8705313205718994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 5/1184 [00:01<05:38,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1041],\n",
      "        [-0.0658]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7365599274635315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 6/1184 [00:01<05:58,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2201],\n",
      "        [-0.0304]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6487880349159241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 7/1184 [00:02<06:34,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2253],\n",
      "        [0.1831]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.596288800239563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 8/1184 [00:02<05:50,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1193],\n",
      "        [-0.0595]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.679308295249939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 9/1184 [00:02<05:20,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0522],\n",
      "        [-0.0821]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7273286581039429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 10/1184 [00:02<04:59,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0536],\n",
      "        [ 0.0987]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6826472878456116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 11/1184 [00:03<04:44,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0077],\n",
      "        [ 0.0105]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6924609541893005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 12/1184 [00:03<04:33,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2158],\n",
      "        [-0.2326]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.8115058541297913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 13/1184 [00:03<04:28,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0184],\n",
      "        [-0.1174]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7187960147857666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1184 [00:03<05:13,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0726],\n",
      "        [0.1059]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6495566368103027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.Adam(vrs1_model.parameters(), lr=0.001)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvrs1_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(model, train_loader, optimizer, loss_fn, config)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):          \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#Question Image Feature\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_img\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_img\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#Answer1 Image Feature\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mSimilarity1_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m q_img_feature \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(q_img)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m#이미지 데이터를 RGB형태로 읽음 \u001b[39;00m\n\u001b[1;32m     22\u001b[0m q_img_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(q_img_feature)  \u001b[38;5;66;03m#이미지 데이터의 크기 및 각도등을 변경\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m a1_img_feature \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(Image\u001b[38;5;241m.\u001b[39mopen(q_img)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m a1_img]\n\u001b[1;32m     25\u001b[0m a2_img_feature \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(Image\u001b[38;5;241m.\u001b[39mopen(q_img)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m a1_img]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_img\u001b[39m\u001b[38;5;124m\"\u001b[39m: q_img_feature,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma1_imgs\u001b[39m\u001b[38;5;124m\"\u001b[39m: a1_img_feature,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma2_imgs\u001b[39m\u001b[38;5;124m\"\u001b[39m: a2_img_feature\n\u001b[1;32m     32\u001b[0m }\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m q_img_feature \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(q_img)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m#이미지 데이터를 RGB형태로 읽음 \u001b[39;00m\n\u001b[1;32m     22\u001b[0m q_img_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(q_img_feature)  \u001b[38;5;66;03m#이미지 데이터의 크기 및 각도등을 변경\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m a1_img_feature \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_img\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m a1_img]\n\u001b[1;32m     25\u001b[0m a2_img_feature \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(Image\u001b[38;5;241m.\u001b[39mopen(q_img)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m a1_img]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_img\u001b[39m\u001b[38;5;124m\"\u001b[39m: q_img_feature,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma1_imgs\u001b[39m\u001b[38;5;124m\"\u001b[39m: a1_img_feature,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma2_imgs\u001b[39m\u001b[38;5;124m\"\u001b[39m: a2_img_feature\n\u001b[1;32m     32\u001b[0m }\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/transforms/functional.py:153\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    151\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(pic\u001b[38;5;241m.\u001b[39mgetbands()))\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.Adam(vrs1_model.parameters(), lr=0.001)\n",
    "for idx in range(200):\n",
    "    train_fn(vrs1_model, train_loader, optimizer, loss_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760abf52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:58.834461Z",
     "start_time": "2022-07-25T08:28:58.834449Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor(2,1)\n",
    "y = torch.Tensor([1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067aaf14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:58.835490Z",
     "start_time": "2022-07-25T08:28:58.835479Z"
    }
   },
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a44fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:58.836147Z",
     "start_time": "2022-07-25T08:28:58.836136Z"
    }
   },
   "outputs": [],
   "source": [
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "bce(x.squeeze(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae53e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
