{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b86cef",
   "metadata": {},
   "source": [
    "# Visual Reasoning Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4cc32",
   "metadata": {},
   "source": [
    "1. Data preprocessing 어떻게 하면될까?\n",
    " - Image feature를 resnet으로 미리 추출해둘까? --> baseline돌리기 까다로움\n",
    " - DataLoader 구성을 어떻게하면 좋을까? --> DataSet class 안에서 feature를 return 할까? 아니면 ResNet을 밖에둘까\n",
    "\n",
    "\n",
    "2. Baseline Model 설계를 어떻게?\n",
    " - Encoder: image encoding 어떻게 하면 될까? Answer Image 3개를 한꺼번에 encoding? 혹은 각각 encoding 한 후 Weight Sum??\n",
    " - Decoder: Answer 후보 중 정답후보를 어떻게 고를까? Similarity 기준? KL 같은 분포기준? Attention 기반 Scoring??\n",
    "\n",
    "\n",
    "3. SOTA Model 설계를 어떻게? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8025aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:18:30.837508Z",
     "start_time": "2022-07-25T08:18:30.833768Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf287b82",
   "metadata": {},
   "source": [
    "## 1. Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c5de51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:29:54.529288Z",
     "start_time": "2022-07-25T08:29:54.518829Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.font_manager as fm\n",
    "#fm.get_fontconfig_fonts()\n",
    "#font_location = './NanumGothic.ttf'\n",
    "#font_location = 'C:/Windows/Fonts/NanumGothic.ttf' # For Windows\n",
    "#font_name = fm.FontProperties(fname=font_location).get_name()\n",
    "#matplotlib.rc('font', family=font_name)\n",
    "\n",
    "%matplotlib inline\n",
    "# 브라우저에서 바로 이미지를 그린다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "316390cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:30:09.235202Z",
     "start_time": "2022-07-25T08:30:09.228784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnrows,ncols=6,15\\nfig,ax = plt.subplots(nrows,ncols,figsize=(15,6))\\nplt.subplots_adjust(wspace=0, hspace=0) \\nfor i,j in enumerate(malignant[:nrows*ncols]):\\n    fname = os.path.join(imgpath ,j +'.tif')\\n    img = Image.open(fname)\\n    idcol = ImageDraw.Draw(img)\\n    idcol.rectangle(((0,0),(95,95)),outline='red')\\n    plt.subplot(nrows, ncols, i+1) \\n    plt.imshow(np.array(img))\\n    plt.axis('off')\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nrows,ncols=6,15\n",
    "fig,ax = plt.subplots(nrows,ncols,figsize=(15,6))\n",
    "plt.subplots_adjust(wspace=0, hspace=0) \n",
    "for i,j in enumerate(malignant[:nrows*ncols]):\n",
    "    fname = os.path.join(imgpath ,j +'.tif')\n",
    "    img = Image.open(fname)\n",
    "    idcol = ImageDraw.Draw(img)\n",
    "    idcol.rectangle(((0,0),(95,95)),outline='red')\n",
    "    plt.subplot(nrows, ncols, i+1) \n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')\n",
    "    \n",
    "import plotly.express as px\n",
    "\n",
    "# Create grid of sample images \n",
    "grid_size=30\n",
    "rnd_inds=np.random.randint(0,len(train_ts),grid_size)\n",
    "print(\"image indices:\",rnd_inds)\n",
    "\n",
    "x_grid_train=[train_ts[i][0] for i in rnd_inds]\n",
    "y_grid_train=[train_ts[i][1] for i in rnd_inds]\n",
    "\n",
    "x_grid_train=utils.make_grid(x_grid_train, nrow=10, padding=2)\n",
    "print(x_grid_train.shape)\n",
    "    \n",
    "plot_img(x_grid_train,y_grid_train,'Training Subset Examples')\n",
    "image indices: [1438  355 2681 1007 2970 2925  605 1739  270 1641 1080  786 1840   62\n",
    " 2633  734 1712  511 2681 2973  180 2601  500 1959  869    3 2359 3153\n",
    "  345 1307]\n",
    "torch.Size([3, 146, 482])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c84fc7",
   "metadata": {},
   "source": [
    "## 2. Preparation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a0e88e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.399816Z",
     "start_time": "2022-07-25T08:28:54.395693Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args=None):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.distributed = False\n",
    "        self.gpu_id = \"0,1\"\n",
    "        self.HOME_DIR = \"./datasets/\"\n",
    "        self.TASK_NAME = \"similarity1/\"\n",
    "        self.FOLDER_NAME = \"000003/\"\n",
    "        self.IMAGE_LIST = [\n",
    "                \"0d73dee440ef4291ae926fb5cb4ec55e.jpg\", \n",
    "                \"18f827e0d01742d495d3ecbaffb6255a.jpg\", \n",
    "                \"1b243a095866423da8f4a8f19d78ecf4.jpg\",\n",
    "                \"4db9fa5bd947497d91bffd8de3b07e6e.jpg\",\n",
    "                \"95ac10e3608e4abba9407a2a1cae2883.jpg\",\n",
    "                \"a05e9630fd754a249b1fba0be5f386ed.jpg\",\n",
    "                \"ea1f8a29ba464af4b2b393d4bb50d7c0.jpg\"\n",
    "             ]\n",
    "        self.JSON_NAME = \"000003\"+\".json\"\n",
    "        self.input_dim = 512\n",
    "        self.mlp_hidden = 1024\n",
    "\n",
    "\n",
    "#config = Config()\n",
    "#a_image_file = config.HOME_DIR+config.TASK_NAME+config.FOLDER_NAME+config.IMAGE_LIST[0]\n",
    "#a_image = plt.imread(a_image_file)\n",
    "#plt.imshow(a_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b7b3930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.409244Z",
     "start_time": "2022-07-25T08:28:54.402077Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    \n",
    "    home_dir = config.HOME_DIR\n",
    "    task_name = config.TASK_NAME\n",
    "    dir_list = os.listdir(home_dir+task_name)\n",
    "    sample_list = []\n",
    "    \n",
    "    for directory in dir_list:\n",
    "        FOLDER_NAME = directory+\"/\"\n",
    "        JSON_NAME = directory+\".json\"\n",
    "        FILE_PATH = home_dir+task_name+FOLDER_NAME\n",
    "        a_data = json.load(open(FILE_PATH+JSON_NAME))\n",
    "        a_data[\"file_path\"] = FILE_PATH\n",
    "        a_data[\"answer1\"] = [a_data[\"Answers\"][0]]\n",
    "        a_data[\"answer2\"] = [a_data[\"Answers\"][1]]\n",
    "        del a_data[\"Answers\"]\n",
    "        sample_list.append(a_data)\n",
    "        \n",
    "    return sample_list\n",
    "\n",
    "def get_img_argumentation():\n",
    "    #이미지 전처리를 위한 이미지 크기 변환 및 각도조정을 위한 transform 선언\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0333692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.416661Z",
     "start_time": "2022-07-25T08:28:54.410525Z"
    }
   },
   "outputs": [],
   "source": [
    "class Similarity1_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, config=None, transform=None):\n",
    "        self.df = df\n",
    "        self.config = config\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        \n",
    "        target = sample[\"correct_answer_group_ID\"][0]\n",
    "        category = sample[\"category\"]\n",
    "        q_img = sample[\"file_path\"] + sample[\"Questions\"][0][\"images\"][0][\"image_url\"]\n",
    "        a1_img = [sample[\"file_path\"] + ans_img[\"image_url\"] for ans_img in sample[\"answer1\"][0][\"images\"]]\n",
    "        a2_img = [sample[\"file_path\"] + ans_img[\"image_url\"] for ans_img in sample[\"answer2\"][0][\"images\"]]\n",
    "        \n",
    "        q_img_feature = Image.open(q_img).convert('RGB')  #이미지 데이터를 RGB형태로 읽음 \n",
    "        q_img_feature = self.transform(q_img_feature)  #이미지 데이터의 크기 및 각도등을 변경\n",
    "        \n",
    "        a1_img_feature = [self.transform(Image.open(q_img).convert('RGB')) for img in a1_img]\n",
    "        a2_img_feature = [self.transform(Image.open(q_img).convert('RGB')) for img in a1_img]\n",
    "        \n",
    "        return {\n",
    "            \"target\": target,\n",
    "            \"q_img\": q_img_feature,\n",
    "            \"a1_imgs\": a1_img_feature,\n",
    "            \"a2_imgs\": a2_img_feature\n",
    "        }\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "007718eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:54.428803Z",
     "start_time": "2022-07-25T08:28:54.419343Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_sequential(in_channels, out_channels, *args, **kwargs):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "           nn.BatchNorm2d(out_channels),\n",
    "           nn.ReLu(),\n",
    "           nn.MaxPool2d(*args, **kwargs))\n",
    "\n",
    "class VRSimilarity(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VRSimilarity, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.backborne = torchvision.models.resnet50(pretrained=True)\n",
    "        self.backborne.fc = nn.Linear(self.backborne.fc.in_features, self.config.input_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(self.config.input_dim*4, self.config.mlp_hidden),\n",
    "                    #nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(self.config.mlp_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, samples):\n",
    "        \n",
    "        #Question Image Feature\n",
    "        q = self.backborne(samples[\"q_img\"])\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        a1_img1 = self.backborne(samples[\"a1_imgs\"][0])\n",
    "        a1_img2 = self.backborne(samples[\"a1_imgs\"][1])\n",
    "        a1_img3 = self.backborne(samples[\"a1_imgs\"][2])\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        a2_img1 = self.backborne(samples[\"a2_imgs\"][0])\n",
    "        a2_img2 = self.backborne(samples[\"a2_imgs\"][1])\n",
    "        a2_img3 = self.backborne(samples[\"a2_imgs\"][2])\n",
    "        \n",
    "        q_a1 = torch.cat([q, a1_img1, a1_img2, a1_img3], axis=1)\n",
    "        q_a2 = torch.cat([q, a2_img1, a2_img2, a2_img3], axis=1)\n",
    "        \n",
    "        q_a1_logit = self.fc(q_a1)\n",
    "        q_a2_logit = self.fc(q_a2)\n",
    "        \n",
    "        return {\n",
    "            \"q_a1_logit\": q_a1_logit,\n",
    "            \"q_a2_logit\": q_a2_logit\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a32563ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:32.970415Z",
     "start_time": "2022-07-25T08:34:32.955571Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, loss_fn, config):\n",
    "    \n",
    "    total_count_correct = 0\n",
    "    total_num_example = 0\n",
    "    total_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    device = config.device\n",
    "    \n",
    "    for batch in tqdm(train_loader):          \n",
    "        \n",
    "        print(batch[\"q_img\"].size())\n",
    "        \n",
    "        #Question Image Feature\n",
    "        batch[\"q_img\"] = batch[\"q_img\"].to(device)\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        batch[\"a1_imgs\"][0] = batch[\"a1_imgs\"][0].to(device)\n",
    "        batch[\"a1_imgs\"][1] = batch[\"a1_imgs\"][1].to(device)\n",
    "        batch[\"a1_imgs\"][2] = batch[\"a1_imgs\"][2].to(device)\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        batch[\"a2_imgs\"][0] = batch[\"a2_imgs\"][0].to(device)\n",
    "        batch[\"a2_imgs\"][1] = batch[\"a2_imgs\"][1].to(device)\n",
    "        batch[\"a2_imgs\"][2] = batch[\"a2_imgs\"][2].to(device)\n",
    "        logits = model(batch)\n",
    "\n",
    "        target_a1, target_a2 = batch[\"target\"].float().to(device), (batch[\"target\"] == 0).type(torch.float).to(device)\n",
    "        print(logits[\"q_a1_logit\"], logits[\"q_a1_logit\"].size())\n",
    "        print(target_a1, target_a1.size())\n",
    "        #loss_a1 = loss_fn(logits[\"q_a1_logit\"].squeeze() if (len(logits[\"q_a1_logit\"].shape) >= 2) else logits[\"q_a1_logit\"], target_a1)\n",
    "        #loss_a2 = loss_fn(logits[\"q_a2_logit\"].squeeze() if (len(logits[\"q_a2_logit\"].shape) >= 2) else logits[\"q_a2_logit\"], target_a2)\n",
    "        loss_a1 = loss_fn(logits[\"q_a1_logit\"].squeeze(), target_a1)\n",
    "        loss_a2 = loss_fn(logits[\"q_a2_logit\"].squeeze(), target_a2)\n",
    "        print(loss_a1.item())\n",
    "        \n",
    "        loss = loss_a1 + loss_a2\n",
    "        #print(loss.item())\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        predicted_a1 = (torch.sigmoid(logits[\"q_a1_logit\"]) > 0.5).float()\n",
    "        total_count_correct = total_count_correct + torch.sum(predicted_a1.squeeze() == target_a1).item()\n",
    "        total_num_example = total_num_example + target_a1.size(0)\n",
    "        \n",
    "        predicted_a2 = (torch.sigmoid(logits[\"q_a2_logit\"]) > 0.5).float()\n",
    "        total_count_correct = total_count_correct + torch.sum(predicted_a2.squeeze() == target_a2).item()\n",
    "        total_num_example = total_num_example + target_a2.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"LOSS:\", str(sum(total_loss)/total_num_example) + \" Accuracy: \" + str(total_count_correct/total_num_example) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5fa88af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:34.222874Z",
     "start_time": "2022-07-25T08:34:33.706483Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "sample_list = get_data(config)\n",
    "df = pd.DataFrame(sample_list)\n",
    "df.drop(df[df[\"doc_id\"] == \"000769\"].index, inplace=True)\n",
    "df.drop(df[df[\"doc_id\"] == \"000256\"].index, inplace=True)\n",
    "train_df, test_df = train_test_split(df)\n",
    "transform = get_img_argumentation()\n",
    "train_datasets = Similarity1_Dataset(train_df, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=2)\n",
    "\n",
    "vrs1_model = VRSimilarity(config)\n",
    "#vrs1_model = torch.nn.DataParallel(vrs1_model)\n",
    "vrs1_model = vrs1_model.to(config.device)\n",
    "if config.distributed:\n",
    "    #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.gpu])    \n",
    "    model = torch.nn.parallel.DistributedDataParallel(model)    \n",
    "\n",
    "optimizer = torch.optim.Adam(vrs1_model.parameters(), lr=0.0001)\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "083f86b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:36.525781Z",
     "start_time": "2022-07-25T08:34:34.482722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1184 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "tensor([[-0.0004],\n",
      "        [ 0.0129]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6900309324264526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/1184 [00:00<04:12,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/1184 [00:00<04:12,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0931],\n",
      "        [0.2075]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6212156414985657\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/1184 [00:00<04:13,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0985],\n",
      "        [-0.0428]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7291795015335083\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/1184 [00:00<04:13,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1526],\n",
      "        [-0.0521]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7459666728973389\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 5/1184 [00:01<04:17,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0732],\n",
      "        [-0.0064]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.7133954763412476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 6/1184 [00:01<05:20,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "tensor([[0.1318],\n",
      "        [0.1354]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6285785436630249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 7/1184 [00:01<05:38,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "tensor([[0.1970],\n",
      "        [0.0510]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6337491273880005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 7/1184 [00:01<05:32,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "tensor([[ 0.0008],\n",
      "        [-0.0179]], device='cuda:0', grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
      "tensor([1., 1.], device='cuda:0') torch.Size([2])\n",
      "0.6974325776100159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.Adam(vrs1_model.parameters(), lr=0.001)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvrs1_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(model, train_loader, optimizer, loss_fn, config)\u001b[0m\n\u001b[1;32m     47\u001b[0m     total_num_example \u001b[38;5;241m=\u001b[39m total_num_example \u001b[38;5;241m+\u001b[39m target_a2\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOSS:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msum\u001b[39m(total_loss)\u001b[38;5;241m/\u001b[39mtotal_num_example) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(total_count_correct\u001b[38;5;241m/\u001b[39mtotal_num_example) )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.Adam(vrs1_model.parameters(), lr=0.001)\n",
    "for idx in range(200):\n",
    "    train_fn(vrs1_model, train_loader, optimizer, loss_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760abf52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:58.834461Z",
     "start_time": "2022-07-25T08:28:58.834449Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor(2,1)\n",
    "y = torch.Tensor([1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067aaf14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:58.835490Z",
     "start_time": "2022-07-25T08:28:58.835479Z"
    }
   },
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a44fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:28:58.836147Z",
     "start_time": "2022-07-25T08:28:58.836136Z"
    }
   },
   "outputs": [],
   "source": [
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "bce(x.squeeze(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef93423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
