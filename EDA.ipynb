{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b86cef",
   "metadata": {},
   "source": [
    "# Visual Reasoning Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4cc32",
   "metadata": {},
   "source": [
    "1. Data preprocessing 어떻게 하면될까?\n",
    " - Image feature를 resnet으로 미리 추출해둘까? --> baseline돌리기 까다로움\n",
    " - DataLoader 구성을 어떻게하면 좋을까? --> DataSet class 안에서 feature를 return 할까? 아니면 ResNet을 밖에둘까\n",
    "\n",
    "\n",
    "2. Baseline Model 설계를 어떻게?\n",
    " - Encoder: image encoding 어떻게 하면 될까? Answer Image 3개를 한꺼번에 encoding? 혹은 각각 encoding 한 후 Weight Sum??\n",
    " - Decoder: Answer 후보 중 정답후보를 어떻게 고를까? Similarity 기준? KL 같은 분포기준? Attention 기반 Scoring??\n",
    "\n",
    "\n",
    "3. SOTA Model 설계를 어떻게? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8025aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf287b82",
   "metadata": {},
   "source": [
    "## 1. Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5de51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.font_manager as fm\n",
    "#fm.get_fontconfig_fonts()\n",
    "#font_location = './NanumGothic.ttf'\n",
    "#font_location = 'C:/Windows/Fonts/NanumGothic.ttf' # For Windows\n",
    "#font_name = fm.FontProperties(fname=font_location).get_name()\n",
    "#matplotlib.rc('font', family=font_name)\n",
    "\n",
    "%matplotlib inline\n",
    "# 브라우저에서 바로 이미지를 그린다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c84fc7",
   "metadata": {},
   "source": [
    "## 2. Preparation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args=None):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.distributed = False\n",
    "        self.gpu_id = \"0,1\"\n",
    "        self.HOME_DIR = \"./datasets/\"\n",
    "        self.TASK_NAME = \"similarity1/\"\n",
    "        self.FOLDER_NAME = \"000003/\"\n",
    "        self.IMAGE_LIST = [\n",
    "                \"0d73dee440ef4291ae926fb5cb4ec55e.jpg\", \n",
    "                \"18f827e0d01742d495d3ecbaffb6255a.jpg\", \n",
    "                \"1b243a095866423da8f4a8f19d78ecf4.jpg\",\n",
    "                \"4db9fa5bd947497d91bffd8de3b07e6e.jpg\",\n",
    "                \"95ac10e3608e4abba9407a2a1cae2883.jpg\",\n",
    "                \"a05e9630fd754a249b1fba0be5f386ed.jpg\",\n",
    "                \"ea1f8a29ba464af4b2b393d4bb50d7c0.jpg\"\n",
    "             ]\n",
    "        self.JSON_NAME = \"000003\"+\".json\"\n",
    "        self.input_dim = 512\n",
    "        self.mlp_hidden = 1024\n",
    "\n",
    "\n",
    "#config = Config()\n",
    "#a_image_file = config.HOME_DIR+config.TASK_NAME+config.FOLDER_NAME+config.IMAGE_LIST[0]\n",
    "#a_image = plt.imread(a_image_file)\n",
    "#plt.imshow(a_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7b3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    \n",
    "    home_dir = config.HOME_DIR\n",
    "    task_name = config.TASK_NAME\n",
    "    dir_list = os.listdir(home_dir+task_name)\n",
    "    sample_list = []\n",
    "    \n",
    "    for directory in dir_list:\n",
    "        FOLDER_NAME = directory+\"/\"\n",
    "        JSON_NAME = directory+\".json\"\n",
    "        FILE_PATH = home_dir+task_name+FOLDER_NAME\n",
    "        a_data = json.load(open(FILE_PATH+JSON_NAME))\n",
    "        a_data[\"file_path\"] = FILE_PATH\n",
    "        a_data[\"answer1\"] = [a_data[\"Answers\"][0]]\n",
    "        a_data[\"answer2\"] = [a_data[\"Answers\"][1]]\n",
    "        del a_data[\"Answers\"]\n",
    "        sample_list.append(a_data)\n",
    "        \n",
    "    return sample_list\n",
    "\n",
    "def get_img_argumentation():\n",
    "    #이미지 전처리를 위한 이미지 크기 변환 및 각도조정을 위한 transform 선언\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0333692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity1_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, config=None, transform=None):\n",
    "        self.df = df\n",
    "        self.config = config\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        \n",
    "        target = sample[\"correct_answer_group_ID\"][0]\n",
    "        q_img = sample[\"file_path\"] + sample[\"Questions\"][0][\"images\"][0][\"image_url\"]\n",
    "        a1_img = [sample[\"file_path\"] + ans_img[\"image_url\"] for ans_img in sample[\"answer1\"][0][\"images\"]]\n",
    "        a2_img = [sample[\"file_path\"] + ans_img[\"image_url\"] for ans_img in sample[\"answer2\"][0][\"images\"]]\n",
    "        \n",
    "        q_img_feature = Image.open(q_img).convert('RGB')  #이미지 데이터를 RGB형태로 읽음 \n",
    "        q_img_feature = self.transform(q_img_feature)  #이미지 데이터의 크기 및 각도등을 변경\n",
    "        \n",
    "        a1_img_feature = [self.transform(Image.open(q_img).convert('RGB')) for img in a1_img]\n",
    "        a2_img_feature = [self.transform(Image.open(q_img).convert('RGB')) for img in a1_img]\n",
    "        \n",
    "        return {\n",
    "            \"target\": target,\n",
    "            \"q_img\": q_img_feature,\n",
    "            \"a1_imgs\": a1_img_feature,\n",
    "            \"a2_imgs\": a2_img_feature\n",
    "        }\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007718eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential(in_channels, out_channels, *args, **kwargs):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "           nn.BatchNorm2d(out_channels),\n",
    "           nn.ReLu(),\n",
    "           nn.MaxPool2d(*args, **kwargs))\n",
    "\n",
    "class VRSimilarity(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VRSimilarity, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.backborne = torchvision.models.resnet50(pretrained=True)\n",
    "        self.backborne.fc = nn.Linear(self.backborne.fc.in_features, self.config.input_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(self.config.input_dim*4, self.config.mlp_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(self.config.mlp_hidden, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, q_img, a1_img1, a1_img2, a1_img3, a2_img1, a2_img2, a2_img3):\n",
    "        \n",
    "        #Question Image Feature\n",
    "        q_img = self.backborne(q_img)\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        a1_img1 = self.backborne(a1_img1)\n",
    "        a1_img2 = self.backborne(a1_img2)\n",
    "        a1_img3 = self.backborne(a1_img3)\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        a2_img1 = self.backborne(a2_img1)\n",
    "        a2_img2 = self.backborne(a2_img2)\n",
    "        a2_img3 = self.backborne(a2_img3)\n",
    "        \n",
    "        q_a1 = torch.cat([q_img, a1_img1, a1_img2, a1_img3], axis=1)\n",
    "        q_a2 = torch.cat([q_img, a2_img1, a2_img2, a2_img3], axis=1)\n",
    "        \n",
    "        q_a1_logit = self.fc(q_a1)\n",
    "        q_a2_logit = self.fc(q_a2)\n",
    "        \n",
    "        return {\n",
    "            \"q_a1_logit\": q_a1_logit,\n",
    "            \"q_a2_logit\": q_a2_logit\n",
    "        }\n",
    "\n",
    "    '''\n",
    "    def forward(self, samples):\n",
    "        \n",
    "        #Question Image Feature\n",
    "        q = self.backborne(samples[\"q_img\"])\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        a1_img1 = self.backborne(samples[\"a1_imgs\"][0])\n",
    "        a1_img2 = self.backborne(samples[\"a1_imgs\"][1])\n",
    "        a1_img3 = self.backborne(samples[\"a1_imgs\"][2])\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        a2_img1 = self.backborne(samples[\"a2_imgs\"][0])\n",
    "        a2_img2 = self.backborne(samples[\"a2_imgs\"][1])\n",
    "        a2_img3 = self.backborne(samples[\"a2_imgs\"][2])\n",
    "        \n",
    "        q_a1 = torch.cat([q, a1_img1, a1_img2, a1_img3], axis=1)\n",
    "        q_a2 = torch.cat([q, a2_img1, a2_img2, a2_img3], axis=1)\n",
    "        \n",
    "        q_a1_logit = self.fc(q_a1)\n",
    "        q_a2_logit = self.fc(q_a2)\n",
    "        \n",
    "        return {\n",
    "            \"q_a1_logit\": q_a1_logit,\n",
    "            \"q_a2_logit\": q_a2_logit\n",
    "        }\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32563ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, loss_fn, config):\n",
    "    \n",
    "    total_count_correct = 0\n",
    "    total_num_example = 0\n",
    "    total_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    device = config.device\n",
    "    \n",
    "    for batch in tqdm(train_loader):   \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''\n",
    "        #Question Image Feature\n",
    "        batch[\"q_img\"] = batch[\"q_img\"].to(device)\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        batch[\"a1_imgs\"][0] = batch[\"a1_imgs\"][0].to(device)\n",
    "        batch[\"a1_imgs\"][1] = batch[\"a1_imgs\"][1].to(device)\n",
    "        batch[\"a1_imgs\"][2] = batch[\"a1_imgs\"][2].to(device)\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        batch[\"a2_imgs\"][0] = batch[\"a2_imgs\"][0].to(device)\n",
    "        batch[\"a2_imgs\"][1] = batch[\"a2_imgs\"][1].to(device)\n",
    "        batch[\"a2_imgs\"][2] = batch[\"a2_imgs\"][2].to(device)\n",
    "        logits = model(batch)\n",
    "        '''\n",
    "\n",
    "        #Question Image Feature\n",
    "        q_img = batch[\"q_img\"].to(device)\n",
    "        \n",
    "        #Answer1 Image Feature\n",
    "        a1_img1 = batch[\"a1_imgs\"][0].to(device)\n",
    "        a1_img2 = batch[\"a1_imgs\"][1].to(device)\n",
    "        a1_img3 = batch[\"a1_imgs\"][2].to(device)\n",
    "        \n",
    "        #Answer2 Image Feature\n",
    "        a2_img1 = batch[\"a2_imgs\"][0].to(device)\n",
    "        a2_img2 = batch[\"a2_imgs\"][1].to(device)\n",
    "        a2_img3 = batch[\"a2_imgs\"][2].to(device)\n",
    "        logits = model(q_img, a1_img1, a1_img2, a1_img3, a2_img1, a2_img2, a2_img3)\n",
    "        \n",
    "        target_a1, target_a2 = batch[\"target\"].float(), (batch[\"target\"] == 0).type(torch.float)\n",
    "        loss_a1 = loss_fn(torch.sigmoid(logits[\"q_a1_logit\"]), target_a1)\n",
    "        loss_a2 = loss_fn(torch.sigmoid(logits[\"q_a2_logit\"]), target_a2)\n",
    "        loss = loss_a1 + loss_a2\n",
    "        total_loss.append(loss)\n",
    "        \n",
    "        predicted_a1 = (torch.sigmoid(logits[\"q_a1_logit\"]) > 0.5).float()\n",
    "        #print(\"a1_logit\", torch.sigmoid(logits[\"q_a1_logit\"]))\n",
    "        total_count_correct = total_count_correct + torch.sum(predicted_a1.squeeze() == target_a1).item()\n",
    "        total_num_example = total_num_example + target_a1.size(0)\n",
    "        \n",
    "        predicted_a2 = (torch.sigmoid(logits[\"q_a2_logit\"]) > 0.5).float()\n",
    "        #print(\"a2_logit\", torch.sigmoid(logits[\"q_a2_logit\"]))\n",
    "        total_count_correct = total_count_correct + torch.sum(predicted_a2.squeeze() == target_a2).item()\n",
    "        total_num_example = total_num_example + target_a2.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"LOSS:\", str(sum(total_loss)/total_num_example) + \" Accuracy: \" + str(total_count_correct/total_num_example) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    config = Config()\n",
    "    \n",
    "    sample_list = get_data(config)\n",
    "    df = pd.DataFrame(sample_list)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    transform = get_img_argumentation()\n",
    "    train_datasets = Similarity1_Dataset(train_df, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=4)\n",
    "    \n",
    "    vrs1_model = VRSimilarity(config)\n",
    "    #vrs1_model = torch.nn.DataParallel(vrs1_model)\n",
    "    vrs1_model = vrs1_model.to(config.device)\n",
    "    #if config.distributed:\n",
    "        #model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.gpu])    \n",
    "        #model = torch.nn.parallel.DistributedDataParallel(model)    \n",
    "        \n",
    "    optimizer = torch.optim.Adam(vrs1_model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    train_fn(vrs1_model, train_loader, optimizer, loss_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab4a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "sample_list = get_data(config)\n",
    "df = pd.DataFrame(sample_list)\n",
    "train_df, test_df = train_test_split(df)\n",
    "transform = get_img_argumentation()\n",
    "train_datasets = Similarity1_Dataset(train_df, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "245b6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13f58056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = config.device\n",
    "a_model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3ecf965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                   | 0/593 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_582642/2932088219.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mq_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"q_img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kbvqa/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kbvqa/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kbvqa/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kbvqa/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kbvqa/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1922\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_loader):   \n",
    "    q_img = batch[\"q_img\"].to(device)\n",
    "    a_model(q_img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5d792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47f879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbvqa",
   "language": "python",
   "name": "kbvqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
